{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3dde310-1722-445c-b303-2efa3cc69972",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, Trainer, TrainingArguments, EsmForMaskedLM, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "import esm\n",
    "from torch.optim import AdamW\n",
    "import plotly.express as px\n",
    "import random\n",
    "import pandas as pd\n",
    "from Bio import SeqIO\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11b23384-8d98-4e9e-9cbd-397c6c8613e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"facebook/esm2_t33_650M_UR50D\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "\n",
    "model = EsmForMaskedLM.from_pretrained(model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16b22ce9-81e8-4f58-b529-cc29ddebcd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(row): #tokenizer and params including special_tokens_mask required for mlm\n",
    "    return tokenizer(\n",
    "        row['seq'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=566,\n",
    "        return_special_tokens_mask=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be5dcc22-958e-485f-a962-452588cb6161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(module):\n",
    "    for layer in module.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "        else:\n",
    "            reset_weights(layer)  # Recursively go deeper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc8cc5c1-9c37-421e-a1e6-cf894d74ef5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred model to GPU\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "if torch.cuda.is_available():\n",
    "    model =  model.to(device)\n",
    "    print(\"Transferred model to GPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17bc4c53-cba9-4a98-bf01-93da5b38542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset weights\n",
    "reset_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b871240-a49c-4f0c-b265-ace5d3eecb81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EsmForMaskedLM(\n",
       "  (esm): EsmModel(\n",
       "    (embeddings): EsmEmbeddings(\n",
       "      (word_embeddings): Embedding(33, 1280, padding_idx=1)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "      (position_embeddings): Embedding(1026, 1280, padding_idx=1)\n",
       "    )\n",
       "    (encoder): EsmEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-32): 33 x EsmLayer(\n",
       "          (attention): EsmAttention(\n",
       "            (self): EsmSelfAttention(\n",
       "              (query): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (key): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (value): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "              (rotary_embeddings): RotaryEmbedding()\n",
       "            )\n",
       "            (output): EsmSelfOutput(\n",
       "              (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (intermediate): EsmIntermediate(\n",
       "            (dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          )\n",
       "          (output): EsmOutput(\n",
       "            (dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (LayerNorm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (emb_layer_norm_after): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (contact_head): EsmContactPredictionHead(\n",
       "      (regression): Linear(in_features=660, out_features=1, bias=True)\n",
       "      (activation): Sigmoid()\n",
       "    )\n",
       "  )\n",
       "  (lm_head): EsmLMHead(\n",
       "    (dense): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "    (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "    (decoder): Linear(in_features=1280, out_features=33, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ce16e74-e97c-4a2c-a20a-7816b7cc3f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save reset model alone\n",
    "\n",
    "model.save_pretrained(\"/media/spyros/HD-ADU3/spyros/model_weights_topublish/reset_weights_ESM2/esm2_reset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c288d04-ffa5-40ef-8146-e4258c302a87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a45192a-98db-4921-9338-ad3148c5cabc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf57ed7c-d2cb-4dee-aea3-c711b8abedee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f683f8-e61b-4705-adb6-c5aa6445b21e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da235576-3658-4906-8993-0b575c009c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9890"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haall_train_seqs = SeqIO.to_dict(SeqIO.parse('ncbiflu_HA_all_110424_noX_clu99_filt_HA-all.fas', 'fasta'))\n",
    "len(haall_train_seqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26a574f3-fbfc-4a67-898c-3d9038af534c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['seq'],\n",
      "    num_rows: 7912\n",
      "})\n",
      "Dataset({\n",
      "    features: ['seq'],\n",
      "    num_rows: 1978\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ee178262bf465cbb8119231b419732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/7912 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8742e48d5d84e94a756f01a30383b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1978 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seqdat = [str(haall_train_seqs[x].seq) for x in list(haall_train_seqs)]\n",
    "\n",
    "valseqs = random.sample(seqdat, round(0.2*len(seqdat))) \n",
    "trainseqs = list(set(seqdat) - set(valseqs))\n",
    "# trainseqs = random.sample(seqdat, round(0.008*len(seqdat)))\n",
    "\n",
    "tempdf = pd.DataFrame({'seq':trainseqs})\n",
    "tempdf\n",
    "\n",
    "templ = tempdf.seq.tolist()\n",
    "templ\n",
    "train_dataset = Dataset.from_pandas(tempdf[['seq']])\n",
    "print(train_dataset)\n",
    "\n",
    "\n",
    "tempdf = pd.DataFrame({'seq':valseqs})\n",
    "tempdf\n",
    "\n",
    "templ = tempdf.seq.tolist()\n",
    "templ\n",
    "val_dataset = Dataset.from_pandas(tempdf[['seq']])\n",
    "print(val_dataset)\n",
    "\n",
    "train_dataset = train_dataset.map( #apply the tokenizer to the dataset\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=train_dataset.column_names,\n",
    ")\n",
    "\n",
    "val_dataset = val_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=val_dataset.column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c4400a3-31e5-4cd9-81ec-fdec1018f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,return_tensors='pt',mlm_probability=0.15) #provide random masking and return tensors during training per-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b2134fc7-75c8-4fc5-8183-8942899b9711",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"models\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs = 10,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    seed=13,\n",
    "#     fp16=True,\n",
    "    dataloader_num_workers=4,\n",
    "    disable_tqdm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    #tokenizer=tokenizer, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28c74b87-9aa8-4723-8a09-79b308950163",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4950' max='4950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4950/4950 1:25:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.212838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.169100</td>\n",
       "      <td>0.145361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.170100</td>\n",
       "      <td>0.125289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.135200</td>\n",
       "      <td>0.112351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.118800</td>\n",
       "      <td>0.108128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.109800</td>\n",
       "      <td>0.103779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.102700</td>\n",
       "      <td>0.095490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.095900</td>\n",
       "      <td>0.093172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.089500</td>\n",
       "      <td>0.088045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.085000</td>\n",
       "      <td>0.085678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4950, training_loss=0.21688221941090594, metrics={'train_runtime': 5128.9481, 'train_samples_per_second': 15.426, 'train_steps_per_second': 0.965, 'total_flos': 1.7491845197320128e+17, 'train_loss': 0.21688221941090594, 'epoch': 10.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d58617c2-becf-4e40-937a-97d725c25a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"models/esm2_t33-weight_reset-HA_all_110424_clu99_e10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0233a34b-36d8-429a-afac-f34735944517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ac30fe2-f22b-4f71-a569-4cba47bfbb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer,return_tensors='pt',mlm_probability=0.15) #provide random masking and return tensors during training per-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed2bbd46-dedb-4a8e-a97a-ba6120711c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred model to GPU\n",
      "Dataset({\n",
      "    features: ['seq'],\n",
      "    num_rows: 244\n",
      "})\n",
      "Dataset({\n",
      "    features: ['seq'],\n",
      "    num_rows: 61\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a60c8e6487f47c68dda96ab21e343fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/244 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57bc3bf674ed49ac9fa2c9cc4ba0d75a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/61 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 09:35, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.130300</td>\n",
       "      <td>2.918915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.917200</td>\n",
       "      <td>2.908961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.912900</td>\n",
       "      <td>2.905982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.905700</td>\n",
       "      <td>2.908129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.904600</td>\n",
       "      <td>2.895317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.884100</td>\n",
       "      <td>2.844552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.213800</td>\n",
       "      <td>2.898546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.854400</td>\n",
       "      <td>2.815137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.785000</td>\n",
       "      <td>2.751624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.726200</td>\n",
       "      <td>2.701269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** /media/spyros/HD-ADU3/spyros/model_weights_topublish/reset_weights_ESM2_H7/ereset_weights_ESM2_H7_metrics metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               =  5023880GF\n",
      "  train_loss               =     2.9234\n",
      "  train_runtime            = 0:09:42.27\n",
      "  train_samples_per_second =       4.19\n",
      "  train_steps_per_second   =      0.275\n",
      "Transferred model to GPU\n",
      "Dataset({\n",
      "    features: ['seq'],\n",
      "    num_rows: 782\n",
      "})\n",
      "Dataset({\n",
      "    features: ['seq'],\n",
      "    num_rows: 196\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "547999af45264c77970af56fba2b6f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/782 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "472163c4fd594356a7ca67a2e5e2837b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/196 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='490' max='490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [490/490 15:18, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.977400</td>\n",
       "      <td>2.926667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.988500</td>\n",
       "      <td>2.827787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.599000</td>\n",
       "      <td>2.161012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.225700</td>\n",
       "      <td>0.524831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.336800</td>\n",
       "      <td>0.242247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.211400</td>\n",
       "      <td>0.197506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.169200</td>\n",
       "      <td>0.161766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.146900</td>\n",
       "      <td>0.146020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.132800</td>\n",
       "      <td>0.128923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.120800</td>\n",
       "      <td>0.134946</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** /media/spyros/HD-ADU3/spyros/model_weights_topublish/reset_weights_ESM2_H5/ereset_weights_ESM2_H5_metrics metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               = 16101125GF\n",
      "  train_loss               =     1.0909\n",
      "  train_runtime            = 0:15:20.33\n",
      "  train_samples_per_second =      8.497\n",
      "  train_steps_per_second   =      0.532\n",
      "Transferred model to GPU\n",
      "Dataset({\n",
      "    features: ['seq'],\n",
      "    num_rows: 1829\n",
      "})\n",
      "Dataset({\n",
      "    features: ['seq'],\n",
      "    num_rows: 457\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19e5a624ca6e4cabbd425a5d08ae23bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/1829 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9ed79ea024b4cb493a7f6a2418f4230",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/457 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1150' max='1150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1150/1150 26:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.890000</td>\n",
       "      <td>2.500400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.953600</td>\n",
       "      <td>0.279377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.216800</td>\n",
       "      <td>0.185655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.167000</td>\n",
       "      <td>0.154044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.145800</td>\n",
       "      <td>0.135566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.129500</td>\n",
       "      <td>0.121268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.119400</td>\n",
       "      <td>0.121911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>0.109200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.107200</td>\n",
       "      <td>0.105588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.100300</td>\n",
       "      <td>0.103732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** /media/spyros/HD-ADU3/spyros/model_weights_topublish/reset_weights_ESM2_H1/ereset_weights_ESM2_H1_metrics metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               = 37658514GF\n",
      "  train_loss               =     0.4943\n",
      "  train_runtime            = 0:26:24.32\n",
      "  train_samples_per_second =     11.544\n",
      "  train_steps_per_second   =      0.726\n",
      "Transferred model to GPU\n",
      "Dataset({\n",
      "    features: ['seq'],\n",
      "    num_rows: 3393\n",
      "})\n",
      "Dataset({\n",
      "    features: ['seq'],\n",
      "    num_rows: 848\n",
      "})\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2747dcef0a455b9f0f9176e2e069c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/3393 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c4b6ccb2c6b4c9c9d3b5123d5339a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=4):   0%|          | 0/848 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/transformers/training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2130' max='2130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2130/2130 42:49, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.925800</td>\n",
       "      <td>0.264197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.173600</td>\n",
       "      <td>0.142358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.121700</td>\n",
       "      <td>0.115795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.103300</td>\n",
       "      <td>0.105411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.094000</td>\n",
       "      <td>0.096424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.087400</td>\n",
       "      <td>0.096016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.083400</td>\n",
       "      <td>0.086758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.079200</td>\n",
       "      <td>0.087518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.076600</td>\n",
       "      <td>0.086929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.078468</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "/home/spyros/anaconda3/envs/plm_entropy/lib/python3.12/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** /media/spyros/HD-ADU3/spyros/model_weights_topublish/reset_weights_ESM2_H3/ereset_weights_ESM2_H3_metrics metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               = 69860764GF\n",
      "  train_loss               =     0.2815\n",
      "  train_runtime            = 0:42:50.51\n",
      "  train_samples_per_second =       13.2\n",
      "  train_steps_per_second   =      0.829\n"
     ]
    }
   ],
   "source": [
    "#clu99 individual serotypes\n",
    "for sero in ['H7', 'H5', 'H1', 'H3']:\n",
    "\n",
    "    model = EsmForMaskedLM.from_pretrained(model_type)\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    if torch.cuda.is_available():\n",
    "        model =  model.to(device)\n",
    "        print(\"Transferred model to GPU\")\n",
    "\n",
    "    #reset weights\n",
    "    reset_weights(model)\n",
    "\n",
    "    # print(\"\\nPreparing models/esm2_t33-%s_110424_clu99_e10_071024\"%sero)\n",
    "\n",
    "    #load protein sequences from fasta file into a list\n",
    "    seqdat = [str(rec.seq) for rec in SeqIO.parse('../evotuning/ncbiflu_HA_all_110424_noX_clu99_filt_%s.fas'%sero, 'fasta')]\n",
    "\n",
    "    #separate 80-20 training-validation dataset\n",
    "    valseqs = random.sample(seqdat, round(0.2*len(seqdat))) \n",
    "    trainseqs = list(set(seqdat) - set(valseqs))\n",
    "    \n",
    "    tempdf = pd.DataFrame({'seq':trainseqs})\n",
    "    templ = tempdf.seq.tolist()\n",
    "    templ\n",
    "    train_dataset = Dataset.from_pandas(tempdf[['seq']])\n",
    "    print(train_dataset)\n",
    "    \n",
    "    tempdf = pd.DataFrame({'seq':valseqs})\n",
    "    tempdf\n",
    "    templ = tempdf.seq.tolist()\n",
    "    templ\n",
    "    val_dataset = Dataset.from_pandas(tempdf[['seq']])\n",
    "    print(val_dataset)\n",
    "\n",
    "    #apply the tokenizer to the dataset\n",
    "    train_dataset = train_dataset.map( \n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=train_dataset.column_names,\n",
    "    )\n",
    "    \n",
    "    val_dataset = val_dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=4,\n",
    "        remove_columns=val_dataset.column_names,\n",
    "    )\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"/media/spyros/HD-ADU3/spyros/model_weights_topublish/reset_weights_ESM2_%s/\"%sero,\n",
    "        overwrite_output_dir=True,\n",
    "        num_train_epochs = 10,\n",
    "        # logging_steps=100,\n",
    "        logging_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        per_device_train_batch_size=4,\n",
    "        per_device_eval_batch_size=8,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_total_limit=5,\n",
    "        seed=13,\n",
    "    #     fp16=True,\n",
    "        dataloader_num_workers=4,\n",
    "        disable_tqdm=False\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        #tokenizer=tokenizer, \n",
    "    )\n",
    "    \n",
    "    train_result = trainer.train()\n",
    "    # trainer.save_model(\"/data2/spyros/flu_LLM_evol_data/ncbiflu_models_071024/esm2_t33-%s_110424_clu99_e10_071024\"%sero)\n",
    "\n",
    "    metrics = train_result.metrics\n",
    "    \n",
    "    # save train results\n",
    "    trainer.log_metrics(\"/media/spyros/HD-ADU3/spyros/model_weights_topublish/reset_weights_ESM2_%s/ereset_weights_ESM2_%s_metrics\"%(sero, sero), metrics)\n",
    "    trainer.save_metrics(\"/media/spyros/HD-ADU3/spyros/model_weights_topublish/reset_weights_ESM2_%s/ereset_weights_ESM2_%s_metrics\"%(sero, sero), metrics)\n",
    "    with open(\"/media/spyros/HD-ADU3/spyros/model_weights_topublish/reset_weights_ESM2_%s/ereset_weights_ESM2_%s.log\"%(sero, sero), 'w') as out:\n",
    "        out.write(str(trainer.state.log_history))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a35728-7b8b-4b44-95f1-5f257b0fb69b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179115bd-2632-4707-8aed-b743cb7508f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
